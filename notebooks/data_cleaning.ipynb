{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Space Biology Data Cleaning\n",
    "\n",
    "This notebook prepares the local experiments CSV for the Space Biology Knowledge Engine by normalizing fields, deriving filters, and saving a cleaned dataset for ingestion into the app backend and analytics panels.\n",
    "\n",
    "It follows OSDR/GeneLab data processing conventions for standardized metadata across studies, assays, organisms, missions, and tissues to enable reliable filtering and cross-study analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports and setup\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Paths\n",
    "REPO_ROOT = Path(os.getcwd()).resolve().parents[0] if (Path(os.getcwd()).name == \"notebooks\") else Path(os.getcwd()).resolve()\n",
    "DATA_DIR = REPO_ROOT / \"data\"\n",
    "RAW_CSV = DATA_DIR / \"experiments.csv\"\n",
    "CLEAN_CSV = DATA_DIR / \"experiments.csv\"  # overwrite in-place for the app\n",
    "\n",
    "# Theming for plots\n",
    "pio.templates.default = \"plotly_dark\"\n",
    "\n",
    "# External integration (environment-configurable)\n",
    "BACKEND_BASE = os.environ.get(\"FRONTEND_BACKEND_URL\", \"http://localhost:5000\")\n",
    "OSDR_BASE = os.environ.get(\"OSDR_BASE\", \"\")  # leave empty to avoid hardcoding; set in .env to use\n",
    "\n",
    "print(\"Repo root:\", REPO_ROOT)\n",
    "print(\"Data dir:\", DATA_DIR)\n",
    "print(\"Backend base:\", BACKEND_BASE)\n",
    "print(\"OSDR base (optional):\", OSDR_BASE or \"<not set>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "The app expects a CSV with columns such as study_accession, publication_title, year, mission, organism, tissue, assay, platform, environment, doi, source_url, abstract, and metadata_json for quick local development.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RAW_CSV.exists():\n",
    "    raise FileNotFoundError(f\"Missing {RAW_CSV}. Place your experiments.csv in the data/ directory.\")\n",
    "\n", 
    "df = pd.read_csv(RAW_CSV)\n",
    "print(\"Rows loaded:\", len(df))\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning utilities\n",
    "\n",
    "These helpers normalize text, map common organism/assay/mission synonyms to standard labels, and ensure JSON columns are loadable into Python dictionaries for downstream use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def _clean_text(s: Any) -> str:\n",
    "    if pd.isna(s):\n",
    "        return \"\"\n", 
    "    t = str(s)\n",
    "    t = t.replace(\"\\u00a0\", \" \")  # nbsp\n",
    "    t = re.sub(r\"[\\r\\t]+\", \" \", t)\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    return t.strip()\n",
    "\n",
    "ORGANISM_MAP = {\n",
    "    \"mus musculus\": \"Mus musculus\",\n",
    "    \"mouse\": \"Mus musculus\",\n",
    "    \"mice\": \"Mus musculus\",\n",
    "    \"arabidopsis\": \"Arabidopsis thaliana\",\n",
    "    \"arabidopsis thaliana\": \"Arabidopsis thaliana\",\n",
    "    \"drosophila\": \"Drosophila melanogaster\",\n",
    "}\n",
    "\n",
    "ASSAY_MAP = {\n",
    "    \"rna-seq\": \"RNA-seq\",\n",
    "    \"rnaseq\": \"RNA-seq\",\n",
    "    \"microarray\": \"microarray\",\n",
    "    \"amplicon\": \"amplicon\",\n", 
    "    \"methyl-seq\": \"methyl-seq\",\n",
    "}\n",
    "\n",
    "MISSION_MAP = {\n",
    "    \"iss\": \"ISS\",\n",
    "    \"international space station\": \"ISS\",\n",
    "}\n",
    "\n",
    "TISSUE_MAP = {\n",
    "    \"bone\": \"bone\",\n",
    "    \"femur\": \"femur\",\n",
    "    \"leaf\": \"leaf\",\n",
    "}\n",
    "\n",
    "def std_label(s: str, m: Dict[str, str]) -> str | None:\n",
    "    if not s:\n",
    "        return None\n",
    "    low = s.strip().lower()\n",
    "    return m.get(low, s.strip())\n",
    "\n",
    "def parse_metadata_json(x: Any) -> Dict[str, Any]:\n",
    "    if isinstance(x, dict):\n",
    "        return x\n",
    "    if pd.isna(x) or x is None or str(x).strip() == \"\":\n",
    "        return {}\n",
    "    try:\n",
    "        return json.loads(x)\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def coerce_year(v: Any) -> int | None:\n",
    "    try:\n",
    "        iv = int(v)\n",
    "        if 1900 <= iv <= 2100:\n",
    "            return iv\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply cleaning and standardization\n",
    "\n",
    "This step trims whitespace, harmonizes entity names, and creates standardized columns for filters used in the app UI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df.columns.tolist()\n",
    "for c in [\n",
    "    \"study_accession\",\"publication_title\",\"year\",\"mission\",\"organism\",\"tissue\",\n",
    "    \"assay\",\"platform\",\"environment\",\"doi\",\"source_url\",\"abstract\",\"metadata_json\"\n",
    "]:\n",
    "    if c not in df.columns:\n",
    "        df[c] = None\n",
    "\n",
    "# Clean text-like fields\n",
    "for c in [\"publication_title\",\"mission\",\"organism\",\"tissue\",\"assay\",\"platform\",\"environment\",\"doi\",\"source_url\",\"abstract\"]:\n",
    "    df[c] = df[c].map(_clean_text)\n",
    "\n", 
    "# Year coercion\n",
    "df[\"year\"] = df[\"year\"].map(coerce_year)\n",
    "\n",
    "# Standardized labels for filters\n",
    "df[\"organism_std\"] = df[\"organism\"].map(lambda x: std_label(x, ORGANISM_MAP))\n",
    "df[\"assay_std\"] = df[\"assay\"].map(lambda x: std_label(x, ASSAY_MAP))\n",
    "df[\"mission_std\"] = df[\"mission\"].map(lambda x: std_label(x, MISSION_MAP))\n",
    "df[\"tissue_std\"] = df[\"tissue\"].map(lambda x: std_label(x, TISSUE_MAP))\n",
    "\n",
    "# Parse metadata_json into a normalized python dict for downstream logic\n",
    "df[\"metadata_parsed\"] = df[\"metadata_json\"].map(parse_metadata_json)\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deduplicate\n",
    "\n",
    "Drop duplicate rows based on a composite key of study_accession, assay, and tissue to prevent double counting in analytics and graph views.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before = len(df)\n",
    "df = df.drop_duplicates(subset=[\"study_accession\",\"assay\",\"tissue\"]).reset_index(drop=True)\n",
    "after = len(df)\n",
    "print(f\"Deduplicated: {before} -> {after}\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derive filter columns and basic quality checks\n",
    "\n",
    "Ensure year is present when possible and add fallback derivations if needed to support timeline and filter operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If year missing, try very simple heuristic from DOI year fragment (optional)\n",
    "def infer_year_from_doi(doi: str) -> int | None:\n",
    "    if not doi:\n",
    "        return None\n",
    "    m = re.search(r\"(20\\d{2}|19\\d{2})\", doi)\n",
    "    if m:\n",
    "        return coerce_year(m.group(1))\n",
    "    return None\n",
    "\n",
    "mask_missing_year = df[\"year\"].isna()\n",
    "df.loc[mask_missing_year, \"year\"] = df.loc[mask_missing_year, \"doi\"].map(infer_year_from_doi)\n",
    "\n",
    "# Show nulls for key fields\n",
    "key_cols = [\"study_accession\",\"publication_title\",\"year\",\"mission_std\",\"organism_std\",\"tissue_std\",\"assay_std\"]\n",
    "df[key_cols].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save cleaned CSV\n",
    "\n",
    "Write back to data/experiments.csv so the app can read standardized fields for search, graph, and analytics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(CLEAN_CSV, index=False)\n",
    "print(\"Saved:\", CLEAN_CSV, \"rows:\", len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick EDA\n",
    "\n",
    "Basic counts by organism, assay, mission, tissue, and year help validate filter distributions for the dashboard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_from_counts(series: pd.Series, title: str):\n",
    "    if series is None or series.empty:\n",
    "        fig = px.bar(title=title)\n",
    "        fig.add_annotation(text=\"No data\", showarrow=False, x=0.5, y=0.5)\n",
    "        return fig\n",
    "    s = series.dropna().astype(str).value_counts()\n",
    "    fig = px.bar(x=s.index, y=s.values, title=title)\n",
    "    fig.update_layout(xaxis_title=None, yaxis_title=None)\n",
    "    return fig\n",
    "\n",
    "fig_org = bar_from_counts(df[\"organism_std\"], \"By organism\")\n",
    "fig_assay = bar_from_counts(df[\"assay_std\"], \"By assay\")\n",
    "fig_mission = bar_from_counts(df[\"mission_std\"], \"By mission\")\n",
    "fig_tissue = bar_from_counts(df[\"tissue_std\"], \"Top tissues\")\n",
    "\n",
    "fig_org.show()\n",
    "fig_assay.show()\n",
    "fig_mission.show()\n",
    "fig_tissue.show()\n",
    "\n",
    "if \"year\" in df.columns:\n",
    "    y = df[\"year\"].dropna()\n",
    "    try:\n",
    "        y = y.astype(int)\n",
    "        yc = y.value_counts().sort_index()\n",
    "        fig_y = px.bar(x=yc.index.astype(str), y=yc.values, title=\"Studies over time\")\n",
    "        fig_y.update_layout(xaxis_title=\"Year\", yaxis_title=\"Count\")\n",
    "        fig_y.show()\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Fetch from OSDR API (stub)\n",
    "\n",
    "This stub shows how to wire up programmatic ingestion from the OSDR API once a base endpoint is configured in the environment, returning a normalized dataframe compatible with this projectâ€™s CSV schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "\n",
    "def fetch_from_osdr(query: str, limit: int = 25) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Placeholder OSDR ingest:\n",
    "    - Reads OSDR_BASE from environment (.env) to avoid hardcoding URLs.\n",
    "    - Returns a DataFrame with columns aligned to experiments.csv schema where possible.\n",
    "    \"\"\"\n",
    "    base = OSDR_BASE.strip()\n",
    "    if not base:\n",
    "        print(\"OSDR_BASE not set; skipping remote fetch.\")\n",
    "        return pd.DataFrame()\n",
    "    # Example pattern; adapt to the actual OSDR API once available\n",
    "    # params = {\"q\": query, \"size\": limit}\n",
    "    # url = f\"{base.rstrip('/')}/search\"\n",
    "    # with httpx.Client(timeout=20) as client:\n",
    "    #     r = client.get(url, params=params)\n",
    "    #     r.raise_for_status()\n",
    "    #     payload = r.json()\n",
    "    # Parse payload into rows\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    # for item in payload.get(\"results\", []):\n",
    "    #     rows.append({ ... map to schema ... })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Example usage (disabled):\n",
    "# df_remote = fetch_from_osdr(\"bone microgravity\", limit=10)\n",
    "# display(df_remote.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Post cleaned data to backend /ingest\n",
    "\n",
    "This helper can push the cleaned CSV to the running backend for indexing, entity extraction, and graph building from within the notebook during development.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_to_backend_ingest(csv_path: Path, mode: str = \"append\") -> dict:\n",
    "    url = f\"{BACKEND_BASE.rstrip('/')}/ingest\"\n",
    "    payload = {\"sources\": [str(csv_path)], \"mode\": mode}\n",
    "    try:\n",
    "        r = httpx.post(url, json=payload, timeout=60)\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "    except Exception as e:\n",
    "        print(\"Ingest failed:\", e)\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "# Example usage (disabled):\n",
    "# resp = post_to_backend_ingest(CLEAN_CSV, mode=\"rebuild\")\n",
    "# resp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

